"""
POST /v1/projects/adroit-hall-301111/locations/us-central1/batches/
{
  "pysparkBatch": {
    "jarFileUris": [
      "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.0.jar"
    ],
    "mainPythonFileUri": "gs://dataproc-staging-us-central1-712368347106-boh5iflc/notebooks/jupyter/MLinPython/pyspark/pyspark_batch.py"
  },
  "labels": {},
  "name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-0478",
  "runtimeConfig": {
    "properties": {
      "spark.executor.instances": "2",
      "spark.driver.cores": "4",
      "spark.executor.cores": "4",
      "spark.app.name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-e3c0"
    }
  },
  "environmentConfig": {
    "executionConfig": {
      "subnetworkUri": "default"
    }
  }
}

{
  "name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-5ecc",
  "uuid": "c4641a46-b618-4faf-bcb3-1e7b756b4c19",
  "createTime": "2022-06-13T08:34:34.142698Z",
  "pysparkBatch": {
    "mainPythonFileUri": "gs://dataproc-staging-us-central1-712368347106-boh5iflc/notebooks/jupyter/MLinPython/pyspark/pyspark_batch.py",
    "jarFileUris": [
      "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.25.0.jar"
    ]
  },
  "runtimeInfo": {
    "outputUri": "gs://dataproc-staging-us-central1-712368347106-boh5iflc/google-cloud-dataproc-metainfo/c6920a6a-ae71-436a-839c-c502eef1681c/jobs/srvls-batch-c4641a46-b618-4faf-bcb3-1e7b756b4c19/driveroutput"
  },
  "state": "SUCCEEDED",
  "stateTime": "2022-06-13T08:36:10.943271Z",
  "creator": "yipkwunleong@gmail.com",
  "runtimeConfig": {
    "properties": {
      "spark:spark.executor.instances": "2",
      "spark:spark.driver.cores": "4",
      "spark:spark.executor.cores": "4",
      "spark:spark.app.name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-e3c0"
    }
  },
  "environmentConfig": {
    "executionConfig": {
      "subnetworkUri": "default"
    },
    "peripheralsConfig": {
      "sparkHistoryServerConfig": {}
    }
  },
  "operation": "projects/adroit-hall-301111/regions/us-central1/operations/06554303-d0d6-4f16-b5c1-a0efd2e77be4",
  "stateHistory": [
    {
      "state": "PENDING",
      "stateStartTime": "2022-06-13T08:34:34.142698Z"
    },
    {
      "state": "RUNNING",
      "stateStartTime": "2022-06-13T08:35:16.880264Z"
    }
  ]
}

*** Failed to verify remote log exists s3:///dag_id=dataproc_automl/run_id=manual__2022-06-15T13:48:30.888789+00:00/task_id=feature_engineering/attempt=1.log.
Please provide a bucket_name instead of "s3:///dag_id=dataproc_automl/run_id=manual__2022-06-15T13:48:30.888789+00:00/task_id=feature_engineering/attempt=1.log"
*** Falling back to local log
*** Reading local file: /usr/local/airflow/logs/dag_id=dataproc_automl/run_id=manual__2022-06-15T13:48:30.888789+00:00/task_id=feature_engineering/attempt=1.log
[2022-06-15, 21:48:32 HKT] {base_task_runner.py:70} DEBUG - Planning to run as the  user
[2022-06-15, 21:48:32 HKT] {taskinstance.py:860} DEBUG - Refreshing TaskInstance <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> from DB
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Task Instance Not Running' PASSED: True, Task is not in running state.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Task Instance State' PASSED: True, Task state queued was valid.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1160} INFO - Dependencies all met for <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]>
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Trigger Rule' PASSED: True, The task instance did not have any upstream tasks.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Task Concurrency' PASSED: True, Task concurrency is not set.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]> dependency 'Pool Slots Available' PASSED: True, There are enough open slots in default_pool to execute the task
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1160} INFO - Dependencies all met for <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [queued]>
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1357} INFO - 
--------------------------------------------------------------------------------
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1358} INFO - Starting attempt 1 of 1
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1359} INFO - 
--------------------------------------------------------------------------------
[2022-06-15, 21:48:32 HKT] {events.py:43} DEBUG - session flush listener: added ['running'] unchanged () deleted ['queued'] - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [running]>
[2022-06-15, 21:48:32 HKT] {manager.py:79} DEBUG - extractor for <class 'airflow.providers.google.cloud.operators.dataproc.DataprocCreateBatchOperator'> is None
[2022-06-15, 21:48:32 HKT] {manager.py:53} WARNING - Unable to find an extractor. task_type=DataprocCreateBatchOperator airflow_dag_id=dataproc_automl task_id=feature_engineering airflow_run_id=manual__2022-06-15T13:48:30.888789+00:00 
[2022-06-15, 21:48:32 HKT] {factory.py:116} ERROR - Did not find openlineage.yml and OPENLINEAGE_URL is not set
[2022-06-15, 21:48:32 HKT] {factory.py:38} WARNING - Couldn't initialize transport; will print events to console.
[2022-06-15, 21:48:32 HKT] {console.py:21} INFO - {"eventTime": "2022-06-15T13:48:32.711621Z", "eventType": "START", "inputs": [], "job": {"facets": {}, "name": "dataproc_automl.feature_engineering", "namespace": "default"}, "outputs": [], "producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "run": {"facets": {"airflow_runArgs": {"_producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "_schemaURL": "https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet", "externalTrigger": true}, "airflow_version": {"_producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "_schemaURL": "https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet", "airflowVersion": "2.3.1+astro.1", "openlineageAirflowVersion": "0.8.2", "operator": "airflow.models.taskinstance.TaskInstance", "taskInfo": "{'_sa_instance_state': <sqlalchemy.orm.state.InstanceState object at 0x7f63749f57f0>, 'dag_id': 'dataproc_automl', 'hostname': '87839358f0d6', 'operator': 'DataprocCreateBatchOperator', 'run_id': 'manual__2022-06-15T13:48:30.888789+00:00', 'unixname': 'astro', 'queued_dttm': datetime.datetime(2022, 6, 15, 13, 48, 31, 496197, tzinfo=Timezone('UTC')), 'next_method': None, 'map_index': -1, 'job_id': 221, 'queued_by_job_id': 167, 'next_kwargs': None, 'start_date': datetime.datetime(2022, 6, 15, 13, 48, 32, 711621, tzinfo=Timezone('UTC')), 'pool': 'default_pool', 'pid': None, 'pool_slots': 1, '_try_number': 1, 'end_date': None, 'executor_config': {}, 'duration': None, 'queue': 'default', 'external_executor_id': None, 'task_id': 'feature_engineering', 'state': <TaskInstanceState.RUNNING: 'running'>, 'priority_weight': 3, 'trigger_id': None, 'max_tries': 0, 'trigger_timeout': None, 'dag_run': <DagRun dataproc_automl @ 2022-06-15 13:48:30.888789+00:00: manual__2022-06-15T13:48:30.888789+00:00, externally triggered: True>, 'rendered_task_instance_fields': None, '_log': <Logger airflow.task (DEBUG)>, 'test_mode': False, 'task': <Task(DataprocCreateBatchOperator): feature_engineering>}"}, "nominalTime": {"_producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "_schemaURL": "https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/NominalTimeRunFacet", "nominalStartTime": "2022-06-15T13:48:30.888789Z"}, "parentRun": {"_producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "_schemaURL": "https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/ParentRunFacet", "job": {"name": "dataproc_automl", "namespace": "default"}, "run": {"runId": "183cabda-cb09-3819-bb72-eeb9d0402d0f"}}, "unknownSourceAttribute": {"_producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "_schemaURL": "https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet", "unknownItems": [{"name": "DataprocCreateBatchOperator", "properties": {"_BaseOperator__from_mapped": false, "_BaseOperator__init_kwargs": {"batch": {"labels": {}, "name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-abc"}, "batch_id": "batch_1655300913", "email": ["yipkwunleong@gmail.com"], "email_on_failure": false, "project_id": "adroit-hall-301111", "region": "us-central1", "retries": 0, "start_date": "<<non-serializable: DateTime>>", "task_id": "feature_engineering"}, "_BaseOperator__instantiated": true, "_dag": "<<non-serializable: DAG>>", "_inlets": [], "_log": "<<non-serializable: Logger>>", "_outlets": [], "batch": {"labels": {}, "name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-abc"}, "batch_id": "batch_1655300913", "depends_on_past": false, "do_xcom_push": true, "downstream_task_ids": ["polling"], "email": ["yipkwunleong@gmail.com"], "email_on_failure": false, "email_on_retry": true, "executor_config": {}, "gcp_conn_id": "google_cloud_default", "ignore_first_depends_on_past": true, "inlets": [], "metadata": [], "outlets": [], "owner": "airflow", "params": "<<non-serializable: ParamsDict>>", "pool": "default_pool", "pool_slots": 1, "priority_weight": 1, "project_id": "adroit-hall-301111", "queue": "default", "region": "us-central1", "retries": 0, "retry_delay": "<<non-serializable: timedelta>>", "retry_exponential_backoff": false, "start_date": "<<non-serializable: DateTime>>", "task_group": "<<non-serializable: weakproxy>>", "task_id": "feature_engineering", "trigger_rule": "all_success", "upstream_task_ids": [], "wait_for_downstream": false, "weight_rule": "downstream"}, "type": "operator"}]}}, "runId": "2e432bf4-b5c5-4a6c-a6c8-d3f0f2de83a1"}}
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1378} INFO - Executing <Task(DataprocCreateBatchOperator): feature_engineering> on 2022-06-15 13:48:30.888789+00:00
[2022-06-15, 21:48:32 HKT] {standard_task_runner.py:52} INFO - Started process 5537 to run task
[2022-06-15, 21:48:32 HKT] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'dataproc_automl', 'feature_engineering', 'manual__2022-06-15T13:48:30.888789+00:00', '--job-id', '221', '--raw', '--subdir', 'DAGS_FOLDER/dataproc_automl.py', '--cfg-path', '/tmp/tmpby5diyp7', '--error-file', '/tmp/tmp4rjqa5lx']
[2022-06-15, 21:48:32 HKT] {standard_task_runner.py:80} INFO - Job 221: Subtask feature_engineering
[2022-06-15, 21:48:32 HKT] {cli_action_loggers.py:66} DEBUG - Calling callbacks: [<function default_action_log at 0x7f634f21eee0>]
[2022-06-15, 21:48:32 HKT] {logging_mixin.py:115} WARNING - /usr/local/lib/python3.9/site-packages/airflow/configuration.py:525 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2022-06-15, 21:48:32 HKT] {settings.py:398} DEBUG - Disposing DB connection pool (PID 5537)
[2022-06-15, 21:48:32 HKT] {settings.py:260} DEBUG - Setting up DB connection pool (PID 5537)
[2022-06-15, 21:48:32 HKT] {settings.py:330} DEBUG - settings.prepare_engine_args(): Using NullPool
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1096} DEBUG - previous_execution_date was called
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1096} DEBUG - previous_execution_date was called
[2022-06-15, 21:48:32 HKT] {task_command.py:370} INFO - Running <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [running]> on host 87839358f0d6
[2022-06-15, 21:48:32 HKT] {taskinstance.py:860} DEBUG - Refreshing TaskInstance <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [running]> from DB
[2022-06-15, 21:48:32 HKT] {events.py:43} DEBUG - session flush listener: added () unchanged ['running'] deleted () - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [running]>
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1096} DEBUG - previous_execution_date was called
[2022-06-15, 21:48:32 HKT] {taskinstance.py:932} DEBUG - Clearing XCom data
[2022-06-15, 21:48:32 HKT] {taskinstance.py:1570} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=yipkwunleong@gmail.com
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=dataproc_automl
AIRFLOW_CTX_TASK_ID=feature_engineering
AIRFLOW_CTX_EXECUTION_DATE=2022-06-15T13:48:30.888789+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-06-15T13:48:30.888789+00:00
[2022-06-15, 21:48:32 HKT] {__init__.py:146} DEBUG - Preparing lineage inlets and outlets
[2022-06-15, 21:48:32 HKT] {__init__.py:190} DEBUG - inlets: [], outlets: []
[2022-06-15, 21:48:33 HKT] {requests.py:182} DEBUG - Making request: POST https://oauth2.googleapis.com/token
[2022-06-15, 21:48:33 HKT] {connectionpool.py:1001} DEBUG - Starting new HTTPS connection (1): oauth2.googleapis.com:443
[2022-06-15, 21:48:33 HKT] {connectionpool.py:456} DEBUG - https://oauth2.googleapis.com:443 "POST /token HTTP/1.1" 200 None
[2022-06-15, 21:48:33 HKT] {secret_manager_client.py:85} ERROR - Google Cloud API Call Error (NotFound): Secret ID airflow-connections-google_cloud_default not found.
[2022-06-15, 21:48:33 HKT] {base.py:68} INFO - Using connection ID 'google_cloud_default' for task execution.
[2022-06-15, 21:48:33 HKT] {dataproc.py:2063} INFO - Creating batch
[2022-06-15, 21:48:33 HKT] {credentials_provider.py:312} INFO - Getting connection using `google.auth.default()` since no key file is defined for hook.
[2022-06-15, 21:48:33 HKT] {_default.py:206} DEBUG - Checking /usr/local/airflow/include/credentials/adroit-hall-301111-abf4a2e16974.json for explicit credentials as part of auth process...
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/google/api_core/grpc_helpers.py", line 67, in error_remapped_callable
    return callable_(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/grpc/_channel.py", line 946, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File "/usr/local/lib/python3.9/site-packages/grpc/_channel.py", line 849, in _end_unary_response_blocking
    raise _InactiveRpcError(state)
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
	status = StatusCode.INVALID_ARGUMENT
	details = "Multiple validation errors:
 - A batch config must be specified
 - Batch ID 'batch_1655300913' must conform to pattern '[a-z0-9][a-z0-9\-]{2,61}[a-z0-9]'
 - If provided, the batch ID implied by CreateBatchRequest.batch.name must match the batch ID provided in CreateBatchRequest.batch_id"
	debug_error_string = "{"created":"@1655300915.036466700","description":"Error received from peer ipv4:142.250.204.74:443","file":"src/core/lib/surface/call.cc","file_line":952,"grpc_message":"Multiple validation errors:\n - A batch config must be specified\n - Batch ID 'batch_1655300913' must conform to pattern '[a-z0-9][a-z0-9\-]{2,61}[a-z0-9]'\n - If provided, the batch ID implied by CreateBatchRequest.batch.name must match the batch ID provided in CreateBatchRequest.batch_id","grpc_status":3}"
>

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/google/cloud/operators/dataproc.py", line 2067, in execute
    self.operation = hook.create_batch(
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/google/common/hooks/base_google.py", line 439, in inner_wrapper
    return func(self, *args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/airflow/providers/google/cloud/hooks/dataproc.py", line 982, in create_batch
    result = client.create_batch(
  File "/usr/local/lib/python3.9/site-packages/google/cloud/dataproc_v1/services/batch_controller/client.py", line 555, in create_batch
    response = rpc(
  File "/usr/local/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py", line 145, in __call__
    return wrapped_func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/google/api_core/grpc_helpers.py", line 69, in error_remapped_callable
    six.raise_from(exceptions.from_grpc_error(exc), exc)
  File "<string>", line 3, in raise_from
google.api_core.exceptions.InvalidArgument: 400 Multiple validation errors:
 - A batch config must be specified
 - Batch ID 'batch_1655300913' must conform to pattern '[a-z0-9][a-z0-9\-]{2,61}[a-z0-9]'
 - If provided, the batch ID implied by CreateBatchRequest.batch.name must match the batch ID provided in CreateBatchRequest.batch_id
[2022-06-15, 21:48:35 HKT] {taskinstance.py:860} DEBUG - Refreshing TaskInstance <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [running]> from DB
[2022-06-15, 21:48:35 HKT] {taskinstance.py:2339} DEBUG - Task Duration set to 2.337076
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1407} DEBUG - Clearing next_method and next_kwargs.
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1396} INFO - Marking task as FAILED. dag_id=dataproc_automl, task_id=feature_engineering, execution_date=20220615T134830, start_date=20220615T134832, end_date=20220615T134835
[2022-06-15, 21:48:35 HKT] {events.py:43} DEBUG - session flush listener: added ['failed'] unchanged () deleted ['running'] - <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [failed]>
[2022-06-15, 21:48:35 HKT] {manager.py:79} DEBUG - extractor for <class 'airflow.providers.google.cloud.operators.dataproc.DataprocCreateBatchOperator'> is None
[2022-06-15, 21:48:35 HKT] {manager.py:53} WARNING - Unable to find an extractor. task_type=DataprocCreateBatchOperator airflow_dag_id=dataproc_automl task_id=feature_engineering airflow_run_id=manual__2022-06-15T13:48:30.888789+00:00 
[2022-06-15, 21:48:35 HKT] {console.py:21} INFO - {"eventTime": "2022-06-15T13:48:35.048697Z", "eventType": "FAIL", "inputs": [], "job": {"facets": {}, "name": "dataproc_automl.feature_engineering", "namespace": "default"}, "outputs": [], "producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "run": {"facets": {"unknownSourceAttribute": {"_producer": "https://github.com/OpenLineage/OpenLineage/tree/0.8.2/integration/airflow", "_schemaURL": "https://raw.githubusercontent.com/OpenLineage/OpenLineage/main/spec/OpenLineage.json#/definitions/BaseFacet", "unknownItems": [{"name": "DataprocCreateBatchOperator", "properties": {"_BaseOperator__from_mapped": false, "_BaseOperator__init_kwargs": {"batch": {"labels": {}, "name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-abc"}, "batch_id": "batch_1655300913", "email": ["yipkwunleong@gmail.com"], "email_on_failure": false, "project_id": "adroit-hall-301111", "region": "us-central1", "retries": 0, "start_date": "<<non-serializable: DateTime>>", "task_id": "feature_engineering"}, "_BaseOperator__instantiated": true, "_dag": "<<non-serializable: DAG>>", "_inlets": [], "_log": "<<non-serializable: Logger>>", "_outlets": [], "batch": {"labels": {}, "name": "projects/adroit-hall-301111/locations/us-central1/batches/batch-abc"}, "batch_id": "batch_1655300913", "depends_on_past": false, "do_xcom_push": true, "downstream_task_ids": ["polling"], "email": ["yipkwunleong@gmail.com"], "email_on_failure": false, "email_on_retry": true, "executor_config": {}, "gcp_conn_id": "google_cloud_default", "ignore_first_depends_on_past": true, "inlets": [], "metadata": [], "outlets": [], "owner": "airflow", "params": "<<non-serializable: ParamsDict>>", "pool": "default_pool", "pool_slots": 1, "priority_weight": 1, "project_id": "adroit-hall-301111", "queue": "default", "region": "us-central1", "retries": 0, "retry_delay": "<<non-serializable: timedelta>>", "retry_exponential_backoff": false, "start_date": "<<non-serializable: DateTime>>", "task_group": "<<non-serializable: weakproxy>>", "task_id": "feature_engineering", "trigger_rule": "all_success", "upstream_task_ids": [], "wait_for_downstream": false, "weight_rule": "downstream"}, "type": "operator"}]}}, "runId": "2e432bf4-b5c5-4a6c-a6c8-d3f0f2de83a1"}}
[2022-06-15, 21:48:35 HKT] {cli_action_loggers.py:84} DEBUG - Calling callbacks: []
[2022-06-15, 21:48:35 HKT] {standard_task_runner.py:92} ERROR - Failed to execute job 221 for task feature_engineering (400 Multiple validation errors:
 - A batch config must be specified
 - Batch ID 'batch_1655300913' must conform to pattern '[a-z0-9][a-z0-9\-]{2,61}[a-z0-9]'
 - If provided, the batch ID implied by CreateBatchRequest.batch.name must match the batch ID provided in CreateBatchRequest.batch_id; 5537)
[2022-06-15, 21:48:35 HKT] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-06-15, 21:48:35 HKT] {taskinstance.py:860} DEBUG - Refreshing TaskInstance <TaskInstance: dataproc_automl.feature_engineering manual__2022-06-15T13:48:30.888789+00:00 [running]> from DB
[2022-06-15, 21:48:35 HKT] {dagrun.py:641} DEBUG - number of tis tasks for <DagRun dataproc_automl @ 2022-06-15 13:48:30.888789+00:00: manual__2022-06-15T13:48:30.888789+00:00, externally triggered: True>: 3 task(s)
[2022-06-15, 21:48:35 HKT] {dagrun.py:657} DEBUG - number of scheduleable tasks for <DagRun dataproc_automl @ 2022-06-15 13:48:30.888789+00:00: manual__2022-06-15T13:48:30.888789+00:00, externally triggered: True>: 2 task(s)
[2022-06-15, 21:48:35 HKT] {taskinstance.py:959} DEBUG - Setting task state for <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [None]> to upstream_failed
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [upstream_failed]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'total': 1, 'successes': 0, 'skipped': 0, 'failed': 1, 'upstream_failed': 0, 'done': 1}, upstream_task_ids={'feature_engineering'}
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1150} DEBUG - Dependencies not met for <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [upstream_failed]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'total': 1, 'successes': 0, 'skipped': 0, 'failed': 1, 'upstream_failed': 0, 'done': 1}, upstream_task_ids={'feature_engineering'}
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [upstream_failed]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [upstream_failed]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [upstream_failed]> dependency 'Ready To Reschedule' PASSED: True, Task is not in reschedule mode.
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.batch_predict_task manual__2022-06-15T13:48:30.888789+00:00 [None]> dependency 'Trigger Rule' PASSED: False, Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'total': 1, 'successes': 0, 'skipped': 0, 'failed': 0, 'upstream_failed': 0, 'done': 0}, upstream_task_ids={'polling'}
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1150} DEBUG - Dependencies not met for <TaskInstance: dataproc_automl.batch_predict_task manual__2022-06-15T13:48:30.888789+00:00 [None]>, dependency 'Trigger Rule' FAILED: Task's trigger rule 'all_success' requires all upstream tasks to have succeeded, but found 1 non-success(es). upstream_tasks_state={'total': 1, 'successes': 0, 'skipped': 0, 'failed': 0, 'upstream_failed': 0, 'done': 0}, upstream_task_ids={'polling'}
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.batch_predict_task manual__2022-06-15T13:48:30.888789+00:00 [None]> dependency 'Not In Retry Period' PASSED: True, The task instance was not marked for retrying.
[2022-06-15, 21:48:35 HKT] {taskinstance.py:1170} DEBUG - <TaskInstance: dataproc_automl.batch_predict_task manual__2022-06-15T13:48:30.888789+00:00 [None]> dependency 'Previous Dagrun State' PASSED: True, The task did not have depends_on_past set.
[2022-06-15, 21:48:35 HKT] {events.py:43} DEBUG - session flush listener: added ['upstream_failed'] unchanged () deleted [None] - <TaskInstance: dataproc_automl.polling manual__2022-06-15T13:48:30.888789+00:00 [upstream_failed]>
[2022-06-15, 21:48:35 HKT] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check